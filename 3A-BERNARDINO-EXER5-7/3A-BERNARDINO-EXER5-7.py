# -*- coding: utf-8 -*-
"""3A-BERNARDINO-EXER5-7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M2aOZHfGu5qQ_yECfEMeJVNUvjEUnAff

# Topic 3.2: Advanced Probabilistic Models
## Markov decision processes

# **Step 1: Set Up the Environment**

Objective: Prepare your Python environment and install the required libraries.
"""

# Install required libraries
import numpy as np
import matplotlib.pyplot as plt

# Define the grid environment parameters
grid_size = 5
start_state = (0, 0)
goal_state = (4, 4)
penalty_states = [(2, 2), (3, 3)]

# Rewards for different states
reward_goal = 10
reward_penalty = -10
reward_step = -1

# Define actions and their effects
actions = ["U", "D", "L", "R"]
action_effects = {
    "U": (-1, 0),  # Up
    "D": (1, 0),   # Down
    "L": (0, -1),  # Left
    "R": (0, 1)    # Right
}

# Transition probabilities
transition_probabilities = {
    "intended": 0.8,
    "left": 0.1,
    "right": 0.1
}

"""# **Step 2: Define Helper Functions**

Objective: Create a Dynamic Bayesian Network (DBN) to model sequential data.
"""

def is_valid(state):
    """Check if the state is within grid boundaries."""
    x, y = state
    return 0 <= x < grid_size and 0 <= y < grid_size

def get_next_state(state, action):
    """Get the next state after taking an action."""
    dx, dy = action_effects[action]
    next_state = (state[0] + dx, state[1] + dy)
    return next_state if is_valid(next_state) else state

def get_reward(state):
    """Return the reward for a given state."""
    if state == goal_state:
        return reward_goal
    if state in penalty_states:
        return reward_penalty
    return reward_step

"""# **Step 3: Value Iteration Implementation**

Objective: Query the DBN model to predict the probabilities of future states.
"""

def value_iteration(discount_factor=0.9, theta=1e-4):
    """Perform Value Iteration to find the optimal policy."""
    # Initialize value function and policy
    V = np.zeros((grid_size, grid_size))
    policy = np.full((grid_size, grid_size), '', dtype=object)

    while True:
        delta = 0  # Maximum change in value function
        new_V = np.copy(V)

        for x in range(grid_size):
            for y in range(grid_size):
                state = (x, y)
                if state == goal_state:
                    continue  # Skip goal state

                action_values = []
                for action in actions:
                    value = 0
                    for prob_type, prob in transition_probabilities.items():
                        if prob_type == "intended":
                            next_action = action
                        elif prob_type == "left":
                            next_action = actions[(actions.index(action) - 1) % 4]
                        elif prob_type == "right":
                            next_action = actions[(actions.index(action) + 1) % 4]

                        next_state = get_next_state(state, next_action)
                        value += prob * (get_reward(next_state) + discount_factor * V[next_state])
                    action_values.append(value)

                # Update value function and policy
                best_action_value = max(action_values)
                new_V[state] = best_action_value
                policy[state] = actions[np.argmax(action_values)]
                delta = max(delta, abs(new_V[state] - V[state]))

        V = new_V
        if delta < theta:  # Convergence condition
            break

    return V, policy

"""# **Step 4: Run Value Iteration**

Objective: Create a visual representation of the DBN structure.
"""

# Run the Value Iteration algorithm
optimal_values, optimal_policy = value_iteration()

# Display the results
print("Optimal Value Function:")
print(optimal_values)

print("\nOptimal Policy:")
for row in optimal_policy:
    print(row)

"""# **Step 5: Visualization**

Objective: Modify the DBN to include additional variables, such as WetGrass.
"""

def plot_policy(policy):
    """Visualize the optimal policy."""
    fig, ax = plt.subplots(figsize=(6, 6))
    for x in range(grid_size):
        for y in range(grid_size):
            if (x, y) == goal_state:
                ax.text(y, x, "G", ha='center', va='center', color='green', fontsize=14)
            elif (x, y) in penalty_states:
                ax.text(y, x, "X", ha='center', va='center', color='red', fontsize=14)
            else:
                ax.text(y, x, policy[(x, y)], ha='center', va='center', fontsize=12)

    ax.set_xticks(np.arange(-0.5, grid_size, 1))
    ax.set_yticks(np.arange(-0.5, grid_size, 1))
    ax.grid(True)
    ax.set_xlim(-0.5, grid_size - 0.5)
    ax.set_ylim(-0.5, grid_size - 0.5)
    plt.gca().invert_yaxis()
    plt.title("Optimal Policy Visualization")
    plt.show()

plot_policy(optimal_policy)

"""**Expected Outputs**
1. **Optimal Value Function:** Displays a 5x5 grid with the maximum cumulative reward for each state.

2. **Optimal Policy:** Prints the best action (↑, ↓, ←, →) for each state in a 5x5 grid.

3. **Visualization:** A plot of the grid, showing:

  * G for the goal state.
  * X for penalty states.
  * Arrows for the best actions.


"""